 ======================================================================
                       Training process started.                       
 ======================================================================
                                                                       
                          2020-12-12  12:34:23                         
                                                                       
 
 Copyright (C) 2015-2018 Nongnuch Artrith and Alexander Urban
 
 This program is distributed in the hope that it will be useful, but
 WITHOUT ANY WARRANTY; without even the implied warranty of
 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 Mozilla Public License, v. 2.0, for more details.
 
 ----------------------------------------------------------------------
                              Parallel run                             
 ----------------------------------------------------------------------
 
 Number of processes : 1
 
 ----------------------------------------------------------------------
                                Networks                               
 ----------------------------------------------------------------------
 
 Restarting the O network from file : O.40t-40t.nn
 
 Number of layers :   4
 
 Number of nodes (without bias) 
 and activation type per layer :
 
       1 :    60
       2 :    40  hyperbolic tangent (tanh)
       3 :    40  hyperbolic tangent (tanh)
       4 :     1  linear function (linear)
 
 Required memory (words) :     486043 (   3797.21 KB)
 
 Total number of weights (incl. bias) :     4121
 
 Restarting the Ti network from file : Ti.40t-40t.nn
 
 Number of layers :   4
 
 Number of nodes (without bias) 
 and activation type per layer :
 
       1 :    60
       2 :    40  hyperbolic tangent (tanh)
       3 :    40  hyperbolic tangent (tanh)
       4 :     1  linear function (linear)
 
 Required memory (words) :     486043 (   3797.21 KB)
 
 Total number of weights (incl. bias) :     4121
 
 ----------------------------------------------------------------------
                            Storing networks                           
 ----------------------------------------------------------------------
 
 Saving the O network to file : O.40t-40t.nn
 Saving the Ti network to file : Ti.40t-40t.nn
 
 ----------------------------------------------------------------------
                           Training set info.                          
 ----------------------------------------------------------------------
 
 Training set file                   : TiO2.train.scaled
 Number of structures in the data set: 7707
 
 Atomic species in training set      : 2
   Species : O Ti
 
 Average energy (eV/atom) : -0.848934
 Minimum energy (eV/atom) : -1.000000
 Maximum energy (eV/atom) : 0.974678
 
 The input and output values have been normalized to [-1.0, 1.0].
 Structures outside of this interval will not be used for training.
   Energy scaling factor: 0.208959
   Atomic energy shift  : -3.785636
 
 ----------------------------------------------------------------------
                            Training details                           
 ----------------------------------------------------------------------
 
 Training method         : Limited Memory BFGS
 
 Number of iterations    : 0
 
 Training structures     : 6937
 Testing  structures     : 770
 
 Testing set (restarted from previous run): 
 
       6       19       21       23       28       33       62       68 
      72       93       99      104      119      120      122      123 
     127      130      147      152      160      161      168      173 
     178      186      201      202      205      206      215      220 
     225      227      233      237      246      260      263      268 
     280      306      318      321      324      328      330      351 
     359      371      389      397      399      404      409      440 
     443      445      457      488      492      501      530      533 
     535      538      573      590      593      602      604      605 
     607      620      621      625      632      636      638      665 
     678      693      698      757      761      766      769      822 
     836      841      855      859      861      871      874      876 
     880      888      890      904      906      912      918      922 
     930      934      941      948      954      962      972      989 
     991      997     1008     1010     1017     1020     1030     1049 
    1079     1085     1092     1095     1100     1104     1107     1110 
    1120     1125     1126     1131     1134     1141     1220     1224 
    1239     1264     1280     1284     1294     1307     1319     1333 
    1339     1349     1353     1355     1358     1364     1365     1375 
    1389     1392     1412     1413     1422     1428     1436     1450 
    1452     1517     1529     1530     1544     1570     1589     1594 
    1610     1622     1634     1639     1644     1652     1674     1699 
    1704     1712     1737     1754     1756     1758     1760     1770 
    1773     1791     1810     1831     1867     1889     1892     1893 
    1895     1898     1911     1912     1914     1919     1920     1925 
    1937     1941     1942     1952     1968     1981     1984     1986 
    1987     1988     1995     2000     2011     2022     2035     2046 
    2061     2072     2074     2077     2110     2129     2142     2143 
    2159     2160     2169     2170     2171     2175     2185     2210 
    2212     2225     2234     2243     2255     2260     2262     2263 
    2286     2303     2326     2327     2342     2349     2353     2375 
    2378     2380     2381     2390     2392     2395     2400     2401 
    2403     2420     2426     2450     2457     2477     2483     2485 
    2529     2545     2551     2575     2597     2605     2616     2620 
    2672     2676     2679     2680     2722     2738     2742     2758 
    2761     2766     2768     2770     2773     2776     2779     2786 
    2794     2806     2830     2835     2876     2884     2885     2890 
    2899     2902     2911     2914     2915     2932     2938     2958 
    2965     2974     2985     2988     2990     3003     3004     3012 
    3013     3029     3032     3052     3053     3061     3063     3068 
    3072     3091     3099     3106     3117     3128     3147     3156 
    3171     3177     3215     3234     3240     3278     3284     3288 
    3296     3310     3330     3334     3351     3356     3359     3360 
    3368     3395     3410     3426     3438     3440     3442     3444 
    3450     3456     3470     3489     3507     3509     3516     3528 
    3542     3603     3604     3611     3613     3619     3627     3648 
    3668     3675     3686     3695     3710     3718     3729     3733 
    3734     3753     3755     3758     3773     3775     3792     3797 
    3817     3819     3841     3851     3855     3871     3881     3898 
    3909     3916     3930     3935     3936     3938     3943     3957 
    3982     3987     4005     4021     4032     4065     4067     4069 
    4073     4084     4089     4093     4099     4107     4109     4119 
    4124     4130     4132     4134     4136     4143     4156     4166 
    4174     4179     4189     4193     4199     4200     4213     4232 
    4238     4245     4250     4261     4262     4263     4267     4278 
    4280     4285     4289     4295     4306     4308     4309     4316 
    4322     4334     4354     4376     4377     4389     4392     4395 
    4426     4445     4455     4463     4475     4481     4488     4498 
    4502     4506     4532     4535     4541     4546     4554     4557 
    4568     4569     4573     4574     4591     4592     4594     4596 
    4604     4617     4625     4635     4639     4642     4649     4669 
    4681     4692     4711     4715     4758     4766     4771     4772 
    4788     4820     4823     4829     4867     4868     4881     4906 
    4913     4914     4922     4959     4963     4979     5044     5045 
    5048     5053     5058     5075     5095     5098     5106     5110 
    5134     5160     5182     5192     5209     5249     5250     5253 
    5256     5262     5277     5280     5337     5346     5368     5414 
    5425     5454     5460     5476     5484     5498     5508     5518 
    5546     5561     5565     5567     5579     5602     5610     5615 
    5622     5639     5668     5673     5676     5677     5678     5681 
    5722     5755     5756     5773     5779     5789     5806     5813 
    5814     5820     5825     5834     5838     5845     5848     5855 
    5864     5867     5875     5877     5885     5894     5898     5909 
    5934     5941     5944     5950     5976     5983     5986     5993 
    6037     6039     6052     6079     6088     6138     6172     6184 
    6188     6196     6225     6231     6235     6238     6243     6253 
    6263     6274     6278     6281     6290     6304     6306     6313 
    6330     6336     6337     6358     6384     6392     6394     6403 
    6422     6430     6434     6441     6449     6455     6466     6467 
    6470     6477     6487     6497     6542     6566     6567     6585 
    6589     6597     6600     6610     6614     6618     6633     6649 
    6663     6665     6667     6686     6688     6698     6715     6716 
    6726     6740     6761     6764     6770     6775     6780     6816 
    6830     6850     6853     6861     6865     6867     6871     6900 
    6915     6920     6924     6945     6947     6951     6956     6972 
    6973     6974     6981     6988     6995     7000     7002     7008 
    7009     7047     7058     7064     7070     7093     7135     7138 
    7139     7147     7156     7180     7190     7198     7203     7206 
    7225     7227     7228     7232     7254     7256     7260     7266 
    7268     7283     7289     7291     7311     7325     7344     7346 
    7351     7375     7377     7383     7421     7429     7447     7449 
    7494     7508     7517     7525     7530     7555     7558     7560 
    7565     7574     7594     7607     7612     7628     7632     7634 
    7636     7643     7653     7660     7661     7671     7681     7688 
    7690     7691     7692     7693     7694     7695     7696     7697 
    7698     7699     7700     7701     7702     7703     7704     7705 
    7706     7707 
 
 Attempting restart of optimization algorithm from file: train.restart
 Restarting L-BFGS.
 
 ----------------------------------------------------------------------
                            Training process                           
 ----------------------------------------------------------------------
 
 Weight optimization for 0 epochs using the Limited Memory BFGS method.
 
 Sampling type               : sequential
 
        |------------TRAIN-----------|  |------------TEST------------|
 epoch             MAE          <RMSE>             MAE          <RMSE>
     0    1.389105E-03    2.006973E-03    2.202159E-03    4.974993E-03 <
 
 Training finished.
 
 ----------------------------------------------------------------------
                         Storing final energies                        
 ----------------------------------------------------------------------
 
 Energies of training structures : energies.train.PROCESS
 Energies of testing structures  : energies.test.PROCESS
 (Manually concatenate the files from different processes.)
 
 Final MAE of training set  =      1.4 meV/atom
 Final MAE of testing set   =      2.2 meV/atom
 
 Final RMSE of training set =      2.0 meV/atom
 Final RMSE of testing set  =      5.0 meV/atom
 
 ----------------------------------------------------------------------
                            Storing networks                           
 ----------------------------------------------------------------------
 
 Saving the O network to file : O.40t-40t.nn
 Saving the Ti network to file : Ti.40t-40t.nn
 
                                                                       
                          2020-12-12  12:35:07                         
                                                                       
 ======================================================================
                     Neural Network training done.                     
 ======================================================================
